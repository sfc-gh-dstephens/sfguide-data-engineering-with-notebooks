{
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "623743e8-2cd7-47c6-99e7-100979384579",
      "metadata": {
        "name": "md_intro",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "# Snowflake Notebook Data Engineering"
    },
    {
      "cell_type": "markdown",
      "id": "9e6273e5-bcf7-4492-92f6-cc161da082c6",
      "metadata": {
        "name": "md_step03",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Setup Snowflake\n\nDuring this step we will create our demo environment. Update the SQL variables below with your GitHub username and Personal Access Token (PAT) as well as with your forked GitHub repository information.\n\n**Important**: Please make sure you have created the `dev` branch in your forked repository before continuing here. For instructions please see [Step 2 in the Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#1)."
    },
    {
      "cell_type": "code",
      "id": "e898c514-831d-4aa7-9697-004994953950",
      "metadata": {
        "language": "sql",
        "name": "sql_set_context",
        "resultVariableName": "dataframe_1",
        "title": "sql_set_context"
      },
      "outputs": [],
      "source": "SET MY_USER = CURRENT_USER();\n\nSET GITHUB_SECRET_USERNAME = 'username';\nSET GITHUB_SECRET_PASSWORD = 'personal access token';\nSET GITHUB_URL_PREFIX = 'https://github.com/username';\nSET GITHUB_REPO_ORIGIN = 'https://github.com/username/sfguide-data-engineering-with-notebooks.git';",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "dc608c96-0957-47e1-8492-bc8d382925e3",
      "metadata": {
        "language": "sql",
        "name": "sql_account_objects",
        "resultVariableName": "dataframe_2",
        "title": "sql_account_objects"
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Create the account level objects (ACCOUNTADMIN part)\n-- ----------------------------------------------------------------------------\n\nUSE ROLE ACCOUNTADMIN;\n\n-- Roles\nCREATE OR REPLACE ROLE DEMO_ROLE;\nGRANT ROLE DEMO_ROLE TO ROLE SYSADMIN;\nGRANT ROLE DEMO_ROLE TO USER IDENTIFIER($MY_USER);\n\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT EXECUTE TASK ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT MONITOR EXECUTION ON ACCOUNT TO ROLE DEMO_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE DEMO_ROLE;\n\n-- Databases\nCREATE OR REPLACE DATABASE DEMO_DB;\nGRANT OWNERSHIP ON DATABASE DEMO_DB TO ROLE DEMO_ROLE;\n\n-- Warehouses\nCREATE OR REPLACE WAREHOUSE DEMO_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\nGRANT OWNERSHIP ON WAREHOUSE DEMO_WH TO ROLE DEMO_ROLE;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "a1e2ae2c-241b-4d8f-aa99-11a35f9833a4",
      "metadata": {
        "language": "sql",
        "name": "sql_database_objects",
        "resultVariableName": "dataframe_3",
        "title": "sql_database_objects"
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Create the database level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE DATABASE DEMO_DB;\n\n-- Schemas\nCREATE OR REPLACE SCHEMA INTEGRATIONS;\nCREATE OR REPLACE SCHEMA DEV_SCHEMA;\nCREATE OR REPLACE SCHEMA PROD_SCHEMA;\n\nUSE SCHEMA INTEGRATIONS;\n\n-- External Financial Data objects\nCREATE OR REPLACE STAGE FINANCIAL_DATA_RAW_STAGE\n    URL = 's3://snowpark-hol-demo/'\n    DIRECTORY = (ENABLE = TRUE)\n    AUTO_REFRESH = TRUE\n;\n\n-- Secrets (schema level)\nCREATE OR REPLACE SECRET DEMO_GITHUB_SECRET\n  TYPE = password\n  USERNAME = $GITHUB_SECRET_USERNAME\n  PASSWORD = $GITHUB_SECRET_PASSWORD;\n\n-- API Integration (account level)\n-- This depends on the schema level secret!\nCREATE OR REPLACE API INTEGRATION DEMO_GITHUB_API_INTEGRATION\n  API_PROVIDER = GIT_HTTPS_API\n  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)\n  ALLOWED_AUTHENTICATION_SECRETS = (DEMO_GITHUB_SECRET)\n  ENABLED = TRUE;\n\n-- Git Repository\nCREATE OR REPLACE GIT REPOSITORY DEMO_GIT_REPO\n  API_INTEGRATION = DEMO_GITHUB_API_INTEGRATION\n  GIT_CREDENTIALS = DEMO_GITHUB_SECRET\n  ORIGIN = $GITHUB_REPO_ORIGIN;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "06f26add-547e-4d60-8897-d5ad79b3311d",
      "metadata": {
        "language": "sql",
        "name": "sql_event_table",
        "resultVariableName": "dataframe_4",
        "title": "sql_event_table"
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Create the event table\n-- ----------------------------------------------------------------------------\nUSE ROLE ACCOUNTADMIN;\n\nCREATE EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\nGRANT SELECT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\nGRANT INSERT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n\nALTER ACCOUNT SET EVENT_TABLE = DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\nALTER DATABASE DEMO_DB SET LOG_LEVEL = INFO;",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "9531119f-76fc-4a2f-a635-a5a7526ac152",
      "metadata": {
        "name": "md_step04_deploy_dev_notebooks",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Deploy to Dev\n\nFinally we will use `EXECUTE IMMEDIATE FROM <file>` along with Jinja templating to deploy the Dev version of our Notebooks. We will directly execute the SQL script `scripts/deploy_notebooks.sql` from our Git repository which has the SQL commands to deploy a Notebook from a Git repo.\n\nSee [EXECUTE IMMEDIATE FROM](https://docs.snowflake.com/en/sql-reference/sql/execute-immediate-from) for more details."
    },
    {
      "cell_type": "code",
      "id": "ad8676d0-7f82-4639-a5e2-29f7f9dca0f5",
      "metadata": {
        "language": "sql",
        "name": "sql_step04_deploy_dev_notebooks",
        "collapsed": false,
        "resultVariableName": "dataframe_5"
      },
      "outputs": [],
      "source": "USE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;\n\nEXECUTE IMMEDIATE FROM @DEMO_GIT_REPO/branches/main/scripts/deploy_notebooks.sql\n    USING (env => 'DEV', branch => 'dev');",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "d3d93974-9a75-46c2-876f-95b6e1562f75",
      "metadata": {
        "name": "md_step06",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Step Load Excel Files\n\nPlease open and run the `DEV_load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `TRANSACTIONS`, `COMPANY` and `PRESS_AND_EARNING`  tables from the staged Excel files."
    },
    {
      "cell_type": "markdown",
      "id": "b5587283-a264-444d-b9ec-55b4d926a5c7",
      "metadata": {
        "name": "md_step07",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Load Daily Transactions and Company Summaries\n\nPlease open and run the `DEV_load_daily_transactions_and_summaries` Notebook. That Notebook will define the pipeline used to create the `DAILY_TRANSACTION` and `COMPANY_SUMMARY` table."
    },
    {
      "cell_type": "code",
      "id": "63bfff6b-067e-4f24-8424-19d0231c0ee1",
      "metadata": {
        "language": "sql",
        "name": "sql_logs",
        "resultVariableName": "dataframe_8",
        "title": "sql_logs"
      },
      "outputs": [],
      "source": "USE ROLE DEMO_ROLE;\nUSE WAREHOUSE DEMO_WH;\nUSE SCHEMA DEMO_DB.INTEGRATIONS;\n\nSELECT TOP 100\n  RECORD['severity_text'] AS SEVERITY,\n  VALUE AS MESSAGE\nFROM\n  DEMO_DB.INTEGRATIONS.DEMO_EVENTS\nWHERE 1 = 1\n  AND SCOPE['name'] = 'demo_logger'\n  AND RECORD_TYPE = 'LOG';",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c873a1db-1fbe-4a44-b02a-03e1b2084cb2",
      "metadata": {
        "name": "md_step08",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Orchestrate Pipelines\n\nIn this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview). The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. For more details see [Managing Snowflake tasks and task graphs with Python](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-managing-tasks).\n\nThis code is also available in the `scripts/deploy_task_dag.py` script which could be used to automate the Task DAG deployment."
    },
    {
      "cell_type": "code",
      "id": "bdac9ad2-2858-4fb7-b3a2-6394db5b0b4c",
      "metadata": {
        "language": "python",
        "name": "py_imports",
        "title": "py_imports"
      },
      "outputs": [],
      "source": "# Import python packages\nfrom snowflake.core import Root\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nsession.use_role(\"DEMO_ROLE\")\nsession.use_warehouse(\"DEMO_WH\")",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cb2bbc8e-c525-4cd0-b147-a832a9060c47",
      "metadata": {
        "language": "python",
        "name": "py_set_env",
        "title": "py_set_env"
      },
      "outputs": [],
      "source": "database_name = \"DEMO_DB\"\nschema_name = \"DEV_SCHEMA\"\n#schema_name = \"PROD_SCHEMA\"\nenv = 'PROD' if schema_name == 'PROD_SCHEMA' else 'DEV'\n\nsession.use_schema(f\"{database_name}.{schema_name}\")",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "0c030976-5888-4d9f-a329-3248b25abd78",
      "metadata": {
        "language": "python",
        "name": "py_create_dag",
        "title": "py_create_dag"
      },
      "outputs": [],
      "source": "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\nfrom datetime import timedelta\n\n# Create the tasks using the DAG API\nwarehouse_name = \"DEMO_WH\"\ndag_name = \"DEMO_DAG\"\n\napi_root = Root(session)\nschema = api_root.databases[database_name].schemas[schema_name]\ndag_op = DAGOperation(schema)\n\n# Define the DAG\nwith DAG(dag_name, schedule=timedelta(days=1), warehouse=warehouse_name) as dag:\n    dag_task1 = DAGTask(\"LOAD_EXCEL_FILES_TASK\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_load_excel_files\"('database={database_name}', 'schema={schema_name}')''', warehouse=warehouse_name)\n    dag_task2 = DAGTask(\"LOAD_DAILY_TRANSACTIONS_AND_SUMMARIES\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_load_daily_transactions_and_summaries\"('database={database_name}', 'schema={schema_name}')''', warehouse=warehouse_name)\n\n    # Define the dependencies between the tasks\n    dag_task1 >> dag_task2 # dag_task1 is a predecessor of dag_task2\n\n# Create the DAG in Snowflake\ndag_op.deploy(dag, mode=\"orreplace\")",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "f560c909-5523-4a12-ad21-0b9044bdaff6",
      "metadata": {
        "language": "python",
        "name": "py_run_dag",
        "title": "py_run_dag"
      },
      "outputs": [],
      "source": "dagiter = dag_op.iter_dags(like='demo_dag%')\nfor dag_name in dagiter:\n    print(dag_name)\n\n#dag_op.run(dag)",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "8ad46ffc-1137-43dc-add8-7fc02914bbaa",
      "metadata": {
        "name": "md_step09",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Deploy to Production\n\nSteps\n1. Make a small change to a notebook and commit it to the dev branch\n1. Go into GitHub and create a PR and Merge to main branch\n1. Review GitHub Actions workflow definition and run results\n1. See new \"PROD_\" versions of the Notebooks\n1. Deploy the production version of the task DAG\n1. Run production version of the task DAG and see new tables created!"
    },
    {
      "cell_type": "markdown",
      "id": "ba497c01-0988-4c07-af66-79ee2918cffa",
      "metadata": {
        "name": "md_step10",
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Teardown\n\nFinally, we will tear down our demo environment."
    },
    {
      "cell_type": "code",
      "id": "f47ca116-4585-4668-bb72-cf74b0e7b587",
      "metadata": {
        "language": "sql",
        "name": "sql_step10",
        "resultVariableName": "dataframe_9"
      },
      "outputs": [],
      "source": "USE ROLE ACCOUNTADMIN;\n\nDROP API INTEGRATION DEMO_GITHUB_API_INTEGRATION;\nDROP DATABASE DEMO_DB;\nDROP WAREHOUSE DEMO_WH;\nDROP ROLE DEMO_ROLE;\n\n-- Remove the \"dev\" branch in your repo",
      "execution_count": null
    }
  ]
}