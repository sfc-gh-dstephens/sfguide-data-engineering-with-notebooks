{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "623743e8-2cd7-47c6-99e7-100979384579",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_intro"
      },
      "source": [
        "# Snowflake Notebook Data Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6273e5-bcf7-4492-92f6-cc161da082c6",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_step03"
      },
      "source": [
        "## Setup Snowflake\n",
        "\n",
        "During this step we will create our demo environment. Update the SQL variables below with your GitHub username and Personal Access Token (PAT) as well as with your forked GitHub repository information.\n",
        "\n",
        "**Important**: Please make sure you have created the `dev` branch in your forked repository before continuing here. For instructions please see [Step 2 in the Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_with_notebooks/index.html?index=..%2F..index#1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e898c514-831d-4aa7-9697-004994953950",
      "metadata": {
        "language": "sql",
        "name": "sql_set_context",
        "resultVariableName": "dataframe_1",
        "title": "sql_set_context"
      },
      "outputs": [],
      "source": [
        "SET MY_USER = CURRENT_USER();\n",
        "\n",
        "SET GITHUB_SECRET_USERNAME = 'username';\n",
        "SET GITHUB_SECRET_PASSWORD = 'personal access token';\n",
        "SET GITHUB_URL_PREFIX = 'https://github.com/username';\n",
        "SET GITHUB_REPO_ORIGIN = 'https://github.com/username/sfguide-data-engineering-with-notebooks.git';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc608c96-0957-47e1-8492-bc8d382925e3",
      "metadata": {
        "language": "sql",
        "name": "sql_account_objects",
        "resultVariableName": "dataframe_2",
        "title": "sql_account_objects"
      },
      "outputs": [],
      "source": [
        "-- ----------------------------------------------------------------------------\n",
        "-- Create the account level objects (ACCOUNTADMIN part)\n",
        "-- ----------------------------------------------------------------------------\n",
        "\n",
        "USE ROLE ACCOUNTADMIN;\n",
        "\n",
        "-- Roles\n",
        "CREATE OR REPLACE ROLE DEMO_ROLE;\n",
        "GRANT ROLE DEMO_ROLE TO ROLE SYSADMIN;\n",
        "GRANT ROLE DEMO_ROLE TO USER IDENTIFIER($MY_USER);\n",
        "\n",
        "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE DEMO_ROLE;\n",
        "GRANT EXECUTE TASK ON ACCOUNT TO ROLE DEMO_ROLE;\n",
        "GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE DEMO_ROLE;\n",
        "GRANT MONITOR EXECUTION ON ACCOUNT TO ROLE DEMO_ROLE;\n",
        "GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE DEMO_ROLE;\n",
        "\n",
        "-- Databases\n",
        "CREATE OR REPLACE DATABASE DEMO_DB;\n",
        "GRANT OWNERSHIP ON DATABASE DEMO_DB TO ROLE DEMO_ROLE;\n",
        "\n",
        "-- Warehouses\n",
        "CREATE OR REPLACE WAREHOUSE DEMO_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\n",
        "GRANT OWNERSHIP ON WAREHOUSE DEMO_WH TO ROLE DEMO_ROLE;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e2ae2c-241b-4d8f-aa99-11a35f9833a4",
      "metadata": {
        "language": "sql",
        "name": "sql_database_objects",
        "resultVariableName": "dataframe_3",
        "title": "sql_database_objects"
      },
      "outputs": [],
      "source": [
        "-- ----------------------------------------------------------------------------\n",
        "-- Create the database level objects\n",
        "-- ----------------------------------------------------------------------------\n",
        "USE ROLE DEMO_ROLE;\n",
        "USE WAREHOUSE DEMO_WH;\n",
        "USE DATABASE DEMO_DB;\n",
        "\n",
        "-- Schemas\n",
        "CREATE OR REPLACE SCHEMA INTEGRATIONS;\n",
        "CREATE OR REPLACE SCHEMA DEV_SCHEMA;\n",
        "CREATE OR REPLACE SCHEMA PROD_SCHEMA;\n",
        "\n",
        "USE SCHEMA INTEGRATIONS;\n",
        "\n",
        "-- External Financial Data objects\n",
        "CREATE OR REPLACE STAGE FINANCIAL_DATA_RAW_STAGE\n",
        "    URL = 's3://snowpark-hol-demo/'\n",
        "    DIRECTORY = (ENABLE = TRUE)\n",
        ";\n",
        "\n",
        "-- Secrets (schema level)\n",
        "CREATE OR REPLACE SECRET DEMO_GITHUB_SECRET\n",
        "  TYPE = password\n",
        "  USERNAME = $GITHUB_SECRET_USERNAME\n",
        "  PASSWORD = $GITHUB_SECRET_PASSWORD;\n",
        "\n",
        "-- API Integration (account level)\n",
        "-- This depends on the schema level secret!\n",
        "CREATE OR REPLACE API INTEGRATION DEMO_GITHUB_API_INTEGRATION\n",
        "  API_PROVIDER = GIT_HTTPS_API\n",
        "  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)\n",
        "  ALLOWED_AUTHENTICATION_SECRETS = (DEMO_GITHUB_SECRET)\n",
        "  ENABLED = TRUE;\n",
        "\n",
        "-- Git Repository\n",
        "CREATE OR REPLACE GIT REPOSITORY DEMO_GIT_REPO\n",
        "  API_INTEGRATION = DEMO_GITHUB_API_INTEGRATION\n",
        "  GIT_CREDENTIALS = DEMO_GITHUB_SECRET\n",
        "  ORIGIN = $GITHUB_REPO_ORIGIN;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f26add-547e-4d60-8897-d5ad79b3311d",
      "metadata": {
        "language": "sql",
        "name": "sql_event_table",
        "resultVariableName": "dataframe_4",
        "title": "sql_event_table"
      },
      "outputs": [],
      "source": [
        "-- ----------------------------------------------------------------------------\n",
        "-- Create the event table\n",
        "-- ----------------------------------------------------------------------------\n",
        "USE ROLE ACCOUNTADMIN;\n",
        "\n",
        "CREATE EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\n",
        "GRANT SELECT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n",
        "GRANT INSERT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n",
        "\n",
        "ALTER ACCOUNT SET EVENT_TABLE = DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\n",
        "ALTER DATABASE DEMO_DB SET LOG_LEVEL = INFO;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9531119f-76fc-4a2f-a635-a5a7526ac152",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_deploy_dev_notebooks"
      },
      "source": [
        "## Deploy to Dev\n",
        "\n",
        "Finally we will use `EXECUTE IMMEDIATE FROM <file>` along with Jinja templating to deploy the Dev version of our Notebooks. We will directly execute the SQL script `scripts/deploy_notebooks.sql` from our Git repository which has the SQL commands to deploy a Notebook from a Git repo.\n",
        "\n",
        "See [EXECUTE IMMEDIATE FROM](https://docs.snowflake.com/en/sql-reference/sql/execute-immediate-from) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8676d0-7f82-4639-a5e2-29f7f9dca0f5",
      "metadata": {
        "collapsed": false,
        "language": "sql",
        "name": "sql_deploy_dev_notebooks",
        "resultVariableName": "dataframe_5"
      },
      "outputs": [],
      "source": [
        "USE ROLE DEMO_ROLE;\n",
        "USE WAREHOUSE DEMO_WH;\n",
        "USE SCHEMA DEMO_DB.INTEGRATIONS;\n",
        "\n",
        "EXECUTE IMMEDIATE FROM @DEMO_GIT_REPO/branches/main/scripts/deploy_notebooks.sql\n",
        "    USING (env => 'DEV', branch => 'dev');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d93974-9a75-46c2-876f-95b6e1562f75",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_load_excel_files"
      },
      "source": [
        "## Step Load Excel Files\n",
        "\n",
        "Please open and run the `DEV_load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `TRANSACTIONS`, `COMPANY` and `PRESS_AND_EARNING`  tables from the staged Excel files."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5587283-a264-444d-b9ec-55b4d926a5c7",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_load_daily_transactions_and_summaries"
      },
      "source": [
        "## Load Daily Transactions and Company Summaries\n",
        "\n",
        "Please open and run the `DEV_load_daily_transactions_and_summaries` Notebook. That Notebook will define the pipeline used to create the `DAILY_TRANSACTION` and `COMPANY_SUMMARY` table."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c873a1db-1fbe-4a44-b02a-03e1b2084cb2",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_orchestrate_pipelines"
      },
      "source": [
        "## Orchestrate Pipelines\n",
        "\n",
        "In this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview). The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. For more details see [Managing Snowflake tasks and task graphs with Python](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-managing-tasks).\n",
        "\n",
        "This code is also available in the `scripts/deploy_task_dag.py` script which could be used to automate the Task DAG deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdac9ad2-2858-4fb7-b3a2-6394db5b0b4c",
      "metadata": {
        "language": "python",
        "name": "py_imports",
        "title": "py_imports"
      },
      "outputs": [],
      "source": [
        "# Import python packages\n",
        "from snowflake.core import Root\n",
        "\n",
        "# We can also use Snowpark for our analyses!\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "\n",
        "session.use_role(\"DEMO_ROLE\")\n",
        "session.use_warehouse(\"DEMO_WH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2bbc8e-c525-4cd0-b147-a832a9060c47",
      "metadata": {
        "language": "python",
        "name": "py_set_env",
        "title": "py_set_env"
      },
      "outputs": [],
      "source": [
        "database_name = \"DEMO_DB\"\n",
        "schema_name = \"DEV_SCHEMA\"\n",
        "#schema_name = \"PROD_SCHEMA\"\n",
        "env = 'PROD' if schema_name == 'PROD_SCHEMA' else 'DEV'\n",
        "\n",
        "session.use_schema(f\"{database_name}.{schema_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c030976-5888-4d9f-a329-3248b25abd78",
      "metadata": {
        "language": "python",
        "name": "py_create_dag",
        "title": "py_create_dag"
      },
      "outputs": [],
      "source": [
        "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
        "from datetime import timedelta\n",
        "\n",
        "# Create the tasks using the DAG API\n",
        "warehouse_name = \"DEMO_WH\"\n",
        "dag_name = \"DEMO_DAG\"\n",
        "\n",
        "api_root = Root(session)\n",
        "schema = api_root.databases[database_name].schemas[schema_name]\n",
        "dag_op = DAGOperation(schema)\n",
        "\n",
        "# Define the DAG\n",
        "with DAG(dag_name, schedule=timedelta(days=1), warehouse=warehouse_name) as dag:\n",
        "    dag_task1 = DAGTask(\"LOAD_EXCEL_FILES_TASK\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_load_excel_files\"('database={database_name}', 'schema={schema_name}')''', warehouse=warehouse_name)\n",
        "    dag_task2 = DAGTask(\"LOAD_DAILY_TRANSACTIONS_AND_SUMMARIES\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_load_daily_transactions_and_summaries\"('database={database_name}', 'schema={schema_name}')''', warehouse=warehouse_name)\n",
        "\n",
        "    # Define the dependencies between the tasks\n",
        "    dag_task1 >> dag_task2 # dag_task1 is a predecessor of dag_task2\n",
        "\n",
        "# Create the DAG in Snowflake\n",
        "dag_op.deploy(dag, mode=\"orreplace\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f560c909-5523-4a12-ad21-0b9044bdaff6",
      "metadata": {
        "language": "python",
        "name": "py_run_dag",
        "title": "py_run_dag"
      },
      "outputs": [],
      "source": [
        "dagiter = dag_op.iter_dags(like='demo_dag%')\n",
        "for dag_name in dagiter:\n",
        "    print(dag_name)\n",
        "\n",
        "#dag_op.run(dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad46ffc-1137-43dc-add8-7fc02914bbaa",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_deploy_to_production"
      },
      "source": [
        "## Deploy to Production\n",
        "\n",
        "Steps\n",
        "1. Make a small change to a notebook and commit it to the dev branch\n",
        "1. Go into GitHub and create a PR and Merge to main branch\n",
        "1. Review GitHub Actions workflow definition and run results\n",
        "1. See new \"PROD_\" versions of the Notebooks\n",
        "1. Deploy the production version of the task DAG\n",
        "1. Run production version of the task DAG and see new tables created!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba497c01-0988-4c07-af66-79ee2918cffa",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_teardown"
      },
      "source": [
        "## Teardown\n",
        "\n",
        "Finally, we will tear down our demo environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47ca116-4585-4668-bb72-cf74b0e7b587",
      "metadata": {
        "language": "sql",
        "name": "sql_teardown",
        "resultVariableName": "dataframe_9"
      },
      "outputs": [],
      "source": [
        "USE ROLE ACCOUNTADMIN;\n",
        "\n",
        "DROP API INTEGRATION DEMO_GITHUB_API_INTEGRATION;\n",
        "DROP DATABASE DEMO_DB;\n",
        "DROP WAREHOUSE DEMO_WH;\n",
        "DROP ROLE DEMO_ROLE;\n",
        "\n",
        "-- Remove the \"dev\" branch in your repo"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
