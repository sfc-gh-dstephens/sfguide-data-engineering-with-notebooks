{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "v56cds36ztoicz3codh4",
   "authorId": "2604475314212",
   "authorName": "DSTEPHENS",
   "authorEmail": "dexter.stephens@snowflake.com",
   "sessionId": "ce1898f0-e690-409c-a4a5-6c958712d5dc",
   "lastEditTime": 1764867677301
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4132a4ef-a90f-4aa4-b334-d7b1aeb2e91e",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "md_overview"
   },
   "source": [
    "# Load Daily Transactions and Summaries\n",
    "\n",
    "* Author: Dexter Stephens\n",
    "* Last Updated: 12/1/2025\n",
    "\n",
    "This notebook will load data into the `DAILY_TRANSACTION` and `COMPANY_SUMMARY` tables with support for incremental processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283d2ff-6b0e-479c-9c1d-3d6066043d04",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "py_imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import sys\n",
    "import logging\n",
    "from snowflake.core import Root\n",
    "\n",
    "logger = logging.getLogger(\"demo_logger\")\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Change the context if arguments have been passed to the execution of the notebook, in the format of \"arg=value\"\n",
    "args = {\n",
    "    'database': session.get_current_database(),\n",
    "    'schema': session.get_current_schema()\n",
    "}\n",
    "\n",
    "for arg in sys.argv:\n",
    "    if arg.find(\"=\") != -1:\n",
    "        key, value = arg.split(\"=\", 1)\n",
    "        if key in args:\n",
    "            args[key] = value\n",
    "\n",
    "\n",
    "session.use_schema(f\"{args['database']}.{args['schema']}\")\n",
    "\n",
    "logger.info(\"load_daily_transactions_and_summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd608eb-bc1f-45a9-81bb-35da23528eed",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "md_function"
   },
   "source": [
    "## Create a function to check if a table exists\n",
    "\n",
    "This function uses the [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7500f-5c4f-4c87-a14f-542427705e07",
   "metadata": {
    "language": "python",
    "name": "py_table_exists"
   },
   "outputs": [],
   "source": [
    "def table_exists(session, database_name='', schema_name='', table_name=''):\n",
    "    root = Root(session)\n",
    "    tables = root.databases[database_name].schemas[schema_name].tables.iter(like=table_name)\n",
    "    for table_obj in tables:\n",
    "        if table_obj.name == table_name:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Not used, SQL alternative to Python version above\n",
    "def table_exists2(session, database_name='', schema_name='', table_name=''):\n",
    "    exists = session.sql(\"SELECT EXISTS (SELECT * FROM {}.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(database_name, schema_name, table_name)).collect()[0]['TABLE_EXISTS']\n",
    "    return exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37822d24-6c8f-4afe-b010-ac1e7f4a9fdf",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "md_pipeline"
   },
   "source": "## Pipeline to create or update Client Portfolio Metrics, Press Sentiment Stock Impact, and Client Sector Risk"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0b39d-d272-46a5-a367-93bccd2f7a80",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "py_process_cpm",
    "title": "py_process_cpm"
   },
   "outputs": [],
   "source": "import snowflake.snowpark.functions as F\n\ntable_name = \"CLIENT_PORTFOLIO_METRICS\"\n\ntransactions = session.table(\"TRANSACTIONS\")\ncompany_data = session.table(\"COMPANY_DATA\")\n\nportfolio = transactions.join(company_data, transactions['TICKER'] == company_data['company_ticker'], 'left')\n\nportfolio_agg = portfolio.group_by(F.col('CLIENT'), F.col('TICKER'), F.col('sector'), F.col('industry')) \\\n    .agg(\n        F.sum(F.when(F.col('POSITION') == 'BUY', F.col('QUANTITY')).otherwise(0) - \n              F.when(F.col('POSITION') == 'SELL', F.col('QUANTITY')).otherwise(0)).alias('NET_POSITION'),\n        F.avg('PRICE').alias('AVG_PRICE'),\n        F.count('*').alias('TRANSACTION_COUNT'),\n        F.max('LASTUPDATED').alias('LAST_TRADE_DATE'),\n        F.avg('market_cap_billions').alias('AVG_MARKET_CAP'),\n        F.avg('pe_ratio').alias('AVG_PE_RATIO')\n    ) \\\n    .select(\n        F.col('CLIENT'),\n        F.col('TICKER'),\n        F.col('sector'),\n        F.col('industry'),\n        F.col('NET_POSITION'),\n        F.round(F.col('AVG_PRICE'), 2).alias('AVG_PRICE'),\n        F.col('TRANSACTION_COUNT'),\n        F.col('LAST_TRADE_DATE'),\n        F.round(F.col('AVG_MARKET_CAP'), 2).alias('AVG_MARKET_CAP'),\n        F.round(F.col('AVG_PE_RATIO'), 2).alias('AVG_PE_RATIO')\n    )\n\nif not table_exists(session, database_name=session.get_current_database(), schema_name=session.get_current_schema(), table_name=table_name):\n    portfolio_agg.write.mode(\"overwrite\").save_as_table(table_name)\n    logger.info(f\"Successfully created {table_name}\")\nelse:\n    cols_to_update = {c: portfolio_agg[c] for c in portfolio_agg.schema.names}\n    target = session.table(table_name)\n    target.merge(portfolio_agg, (target['CLIENT'] == portfolio_agg['CLIENT']) & (target['TICKER'] == portfolio_agg['TICKER']),\n        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n    logger.info(f\"Successfully updated {table_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4774cb-113c-47aa-8ea7-4b9dfc7ff89f",
   "metadata": {
    "language": "python",
    "name": "py_process_pssi",
    "title": "py_process_pssi"
   },
   "outputs": [],
   "source": "table_name = \"PRESS_SENTIMENT_STOCK_IMPACT\"\n\npress = session.table(\"PRESS_AND_EARNINGS\")\ncompany = session.table(\"COMPANY_DATA\")\ntransactions = session.table(\"TRANSACTIONS\")\n\nfrom snowflake.cortex import sentiment\nfrom snowflake.snowpark.functions import lit\n\npress_with_sentiment = press.with_column('SENTIMENT_SCORE', sentiment(F.col('raw_text')))\n\ndaily_sentiment = press_with_sentiment.join(company, press_with_sentiment['company_ticker'] == company['company_ticker']) \\\n    .select(\n        press_with_sentiment['timestamp'],\n        press_with_sentiment['company_ticker'].alias('company_ticker'),\n        press_with_sentiment['source'],\n        company['sector'],\n        F.col('SENTIMENT_SCORE')\n    ) \\\n    .group_by(\n        F.to_date(F.col('timestamp')).alias('DATE'), \n        F.col('company_ticker'), \n        F.col('source'), \n        F.col('sector')\n    ) \\\n    .agg(\n        F.avg('SENTIMENT_SCORE').alias('AVG_SENTIMENT'),\n        F.count('*').alias('PRESS_VOLUME')\n    )\n\ntrading_volume = transactions.group_by(F.to_date(F.col('LASTUPDATED')).alias('DATE'), F.col('TICKER')) \\\n    .agg(\n        F.sum('QUANTITY').alias('TOTAL_VOLUME'),\n        F.avg('PRICE').alias('AVG_STOCK_PRICE')\n    )\n\nfinal_analysis = daily_sentiment.join(trading_volume, \n    (daily_sentiment['DATE'] == trading_volume['DATE']) & \n    (daily_sentiment['company_ticker'] == trading_volume['TICKER']), 'left') \\\n    .select(\n        daily_sentiment['DATE'].alias('DATE'),\n        daily_sentiment['company_ticker'],\n        daily_sentiment['source'],\n        daily_sentiment['sector'],\n        F.round(daily_sentiment['AVG_SENTIMENT'], 3).alias('SENTIMENT_SCORE'),\n        daily_sentiment['PRESS_VOLUME'],\n        F.builtin(\"ZEROIFNULL\")(trading_volume['TOTAL_VOLUME']).alias('TRADING_VOLUME'),\n        F.round(trading_volume['AVG_STOCK_PRICE'], 2).alias('STOCK_PRICE')\n    )\n\nif not table_exists(session, database_name=session.get_current_database(), schema_name=session.get_current_schema(), table_name=table_name):\n    final_analysis.write.mode(\"overwrite\").save_as_table(table_name)\n    logger.info(f\"Successfully created {table_name}\")\nelse:\n    cols_to_update = {c: final_analysis[c] for c in final_analysis.schema.names}\n    target = session.table(table_name)\n    target.merge(final_analysis, \n        (target['DATE'] == final_analysis['DATE']) & \n        (target['company_ticker'] == final_analysis['company_ticker']) &\n        (target['source'] == final_analysis['source']),\n        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n    logger.info(f\"Successfully updated {table_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a799d-a83f-4d53-905a-508cd8322522",
   "metadata": {
    "language": "python",
    "name": "py_process_csr",
    "title": "py_process_csr"
   },
   "outputs": [],
   "source": "table_name = \"CLIENT_SECTOR_RISK\"\n\ntransactions = session.table(\"TRANSACTIONS\")\ncompany = session.table(\"COMPANY_DATA\")\n\nclient_positions = transactions.join(company, transactions['TICKER'] == company['company_ticker']) \\\n    .group_by(F.col('CLIENT'), F.col('sector')) \\\n    .agg(\n        F.sum(F.when(F.col('POSITION') == 'BUY', F.col('PRICE') * F.col('QUANTITY'))\n              .otherwise(F.col('PRICE') * F.col('QUANTITY') * -1)).alias('TOTAL_EXPOSURE'),\n        F.count_distinct('TICKER').alias('UNIQUE_TICKERS'),\n        F.sum('QUANTITY').alias('TOTAL_SHARES')\n    )\n\nclient_totals = transactions.group_by(F.col('CLIENT')) \\\n    .agg(F.sum(F.when(F.col('POSITION') == 'BUY', F.col('PRICE') * F.col('QUANTITY'))\n               .otherwise(F.col('PRICE') * F.col('QUANTITY') * -1)).alias('TOTAL_PORTFOLIO_VALUE'))\n\nrisk_analysis = client_positions.join(client_totals, 'CLIENT') \\\n    .select(\n        F.col('CLIENT'),\n        F.col('sector'),\n        F.round(F.col('TOTAL_EXPOSURE'), 2).alias('SECTOR_EXPOSURE'),\n        F.round((F.col('TOTAL_EXPOSURE') / F.col('TOTAL_PORTFOLIO_VALUE')) * 100, 2).alias('SECTOR_CONCENTRATION_PCT'),\n        F.col('UNIQUE_TICKERS'),\n        F.col('TOTAL_SHARES')\n    )\n\nif not table_exists(session, database_name=session.get_current_database(), schema_name=session.get_current_schema(), table_name=table_name):\n    risk_analysis.write.mode(\"overwrite\").save_as_table(table_name)\n    logger.info(f\"Successfully created {table_name}\")\nelse:\n    cols_to_update = {c: risk_analysis[c] for c in risk_analysis.schema.names}\n    target = session.table(table_name)\n    target.merge(risk_analysis, (target['CLIENT'] == risk_analysis['CLIENT']) & (target['sector'] == risk_analysis['sector']),\n        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n    logger.info(f\"Successfully updated {table_name}\")"
  },
  {
   "cell_type": "markdown",
   "id": "35b06e41-3330-43db-8026-02dfc8d8ecac",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "name": "md_debugging"
   },
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e0cb8-3c80-4bd7-87e2-88526e3377ff",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_debugging",
    "resultVariableName": "dataframe_2"
   },
   "outputs": [],
   "source": [
    "-- SELECT * FROM CLIENT_PORTFOLIO_METRICS LIMIT 10;\n",
    "-- SELECT * FROM PRESS_SENTIMENT_STOCK_IMPACT LIMIT 10;\n",
    "-- SELECT * FROM CLIENT_SECTOR_RISK LIMIT 10;"
   ]
  }
 ]
}