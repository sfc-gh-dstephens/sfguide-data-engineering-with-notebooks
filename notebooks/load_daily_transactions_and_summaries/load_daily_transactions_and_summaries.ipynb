{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4132a4ef-a90f-4aa4-b334-d7b1aeb2e91e",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_overview"
      },
      "source": [
        "# Load Daily Transactions and Summaries\n",
        "\n",
        "* Author: Dexter Stephens\n",
        "* Last Updated: 12/1/2025\n",
        "\n",
        "This notebook will load data into the `DAILY_TRANSACTION` and `COMPANY_SUMMARY` tables with support for incremental processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c47f41d-b110-4662-a907-fb9d0566fe94",
      "metadata": {
        "language": "sql",
        "name": "sql_get_context",
        "resultVariableName": "dataframe_1"
      },
      "outputs": [],
      "source": [
        "-- This won't be needed when we can pass variables to Notebooks!\n",
        "SELECT current_database() AS DATABASE_NAME, current_schema() AS SCHEMA_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2283d2ff-6b0e-479c-9c1d-3d6066043d04",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "py_imports"
      },
      "outputs": [],
      "source": [
        "# Import python packages\n",
        "import logging\n",
        "from snowflake.core import Root\n",
        "\n",
        "logger = logging.getLogger(\"demo_logger\")\n",
        "\n",
        "# Get the target database and schema using the results from the SQL cell above\n",
        "# This won't be needed when we can pass variables to Notebooks!\n",
        "current_context_df = cells.sql_get_context.to_pandas()\n",
        "database_name = current_context_df.iloc[0,0]\n",
        "schema_name = current_context_df.iloc[0,1]\n",
        "\n",
        "# We can also use Snowpark for our analyses!\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "# session.use_schema(f\"{database_name}.{schema_name}\")\n",
        "\n",
        "logger.info(\"load_daily_transactions_and_summaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd608eb-bc1f-45a9-81bb-35da23528eed",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_function"
      },
      "source": [
        "## Create a function to check if a table exists\n",
        "\n",
        "This function uses the [Snowflake Python Management API](https://docs.snowflake.com/en/developer-guide/snowflake-python-api/snowflake-python-overview)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b7500f-5c4f-4c87-a14f-542427705e07",
      "metadata": {
        "language": "python",
        "name": "py_table_exists"
      },
      "outputs": [],
      "source": [
        "def table_exists(session, database_name='', schema_name='', table_name=''):\n",
        "    root = Root(session)\n",
        "    tables = root.databases[database_name].schemas[schema_name].tables.iter(like=table_name)\n",
        "    for table_obj in tables:\n",
        "        if table_obj.name == table_name:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Not used, SQL alternative to Python version above\n",
        "def table_exists2(session, database_name='', schema_name='', table_name=''):\n",
        "    exists = session.sql(\"SELECT EXISTS (SELECT * FROM {}.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(database_name, schema_name, table_name)).collect()[0]['TABLE_EXISTS']\n",
        "    return exists"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37822d24-6c8f-4afe-b010-ac1e7f4a9fdf",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_pipeline"
      },
      "source": [
        "## Pipeline to update daily_transaction table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93b0b39d-d272-46a5-a367-93bccd2f7a80",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "py_process_cpm",
        "title": "py_process_cpm"
      },
      "outputs": [],
      "source": [
        "import snowflake.snowpark.functions as F\n",
        "\n",
        "table_name = \"CLIENT_PORTFOLIO_METRICS\"\n",
        "\n",
        "transactions = session.table(\"TRANSACTIONS\")\n",
        "company_data = session.table(\"COMPANY_DATA\")\n",
        "\n",
        "portfolio = transactions.join(company_data, transactions['TICKER'] == company_data['company_ticker'], 'left')\n",
        "\n",
        "portfolio_agg = portfolio.group_by(F.col('CLIENT'), F.col('TICKER'), F.col('sector'), F.col('industry')) \\\n",
        "    .agg(\n",
        "        F.sum(F.when(F.col('POSITION') == 'BUY', F.col('QUANTITY')).otherwise(0) - \n",
        "              F.when(F.col('POSITION') == 'SELL', F.col('QUANTITY')).otherwise(0)).alias('NET_POSITION'),\n",
        "        F.avg('PRICE').alias('AVG_PRICE'),\n",
        "        F.count('*').alias('TRANSACTION_COUNT'),\n",
        "        F.max('LASTUPDATED').alias('LAST_TRADE_DATE'),\n",
        "        F.avg('market_cap_billions').alias('AVG_MARKET_CAP'),\n",
        "        F.avg('pe_ratio').alias('AVG_PE_RATIO')\n",
        "    ) \\\n",
        "    .select(\n",
        "        F.col('CLIENT'),\n",
        "        F.col('TICKER'),\n",
        "        F.col('sector'),\n",
        "        F.col('industry'),\n",
        "        F.col('NET_POSITION'),\n",
        "        F.round(F.col('AVG_PRICE'), 2).alias('AVG_PRICE'),\n",
        "        F.col('TRANSACTION_COUNT'),\n",
        "        F.col('LAST_TRADE_DATE'),\n",
        "        F.round(F.col('AVG_MARKET_CAP'), 2).alias('AVG_MARKET_CAP'),\n",
        "        F.round(F.col('AVG_PE_RATIO'), 2).alias('AVG_PE_RATIO')\n",
        "    )\n",
        "\n",
        "if not table_exists(session, database_name=database_name, schema_name=schema_name, table_name=table_name):\n",
        "    portfolio_agg.write.mode(\"overwrite\").save_as_table(table_name)\n",
        "    logger.info(f\"Successfully created {table_name}\")\n",
        "else:\n",
        "    cols_to_update = {c: portfolio_agg[c] for c in portfolio_agg.schema.names}\n",
        "    target = session.table(table_name)\n",
        "    target.merge(portfolio_agg, (target['CLIENT'] == portfolio_agg['CLIENT']) & (target['TICKER'] == portfolio_agg['TICKER']),\n",
        "        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n",
        "    logger.info(f\"Successfully updated {table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f4774cb-113c-47aa-8ea7-4b9dfc7ff89f",
      "metadata": {
        "language": "python",
        "name": "py_process_pssi",
        "title": "py_process_pssi"
      },
      "outputs": [],
      "source": [
        "table_name = \"PRESS_SENTIMENT_STOCK_IMPACT\"\n",
        "\n",
        "press = session.table(\"PRESS_AND_EARNINGS\")\n",
        "company = session.table(\"COMPANY_DATA\")\n",
        "transactions = session.table(\"TRANSACTIONS\")\n",
        "\n",
        "from snowflake.cortex import sentiment\n",
        "from snowflake.snowpark.functions import lit\n",
        "\n",
        "press_with_sentiment = press.with_column('SENTIMENT_SCORE', sentiment(F.col('raw_text')))\n",
        "\n",
        "daily_sentiment = press_with_sentiment.join(company, press_with_sentiment['company_ticker'] == company['company_ticker']) \\\n",
        "    .select(\n",
        "        press_with_sentiment['timestamp'],\n",
        "        press_with_sentiment['company_ticker'].alias('company_ticker'),\n",
        "        press_with_sentiment['source'],\n",
        "        company['sector'],\n",
        "        F.col('SENTIMENT_SCORE')\n",
        "    ) \\\n",
        "    .group_by(\n",
        "        F.to_date(F.col('timestamp')).alias('DATE'), \n",
        "        F.col('company_ticker'), \n",
        "        F.col('source'), \n",
        "        F.col('sector')\n",
        "    ) \\\n",
        "    .agg(\n",
        "        F.avg('SENTIMENT_SCORE').alias('AVG_SENTIMENT'),\n",
        "        F.count('*').alias('PRESS_VOLUME')\n",
        "    )\n",
        "\n",
        "trading_volume = transactions.group_by(F.to_date(F.col('LASTUPDATED')).alias('DATE'), F.col('TICKER')) \\\n",
        "    .agg(\n",
        "        F.sum('QUANTITY').alias('TOTAL_VOLUME'),\n",
        "        F.avg('PRICE').alias('AVG_STOCK_PRICE')\n",
        "    )\n",
        "\n",
        "final_analysis = daily_sentiment.join(trading_volume, \n",
        "    (daily_sentiment['DATE'] == trading_volume['DATE']) & \n",
        "    (daily_sentiment['company_ticker'] == trading_volume['TICKER']), 'left') \\\n",
        "    .select(\n",
        "        daily_sentiment['DATE'].alias('DATE'),\n",
        "        daily_sentiment['company_ticker'],\n",
        "        daily_sentiment['source'],\n",
        "        daily_sentiment['sector'],\n",
        "        F.round(daily_sentiment['AVG_SENTIMENT'], 3).alias('SENTIMENT_SCORE'),\n",
        "        daily_sentiment['PRESS_VOLUME'],\n",
        "        F.builtin(\"ZEROIFNULL\")(trading_volume['TOTAL_VOLUME']).alias('TRADING_VOLUME'),\n",
        "        F.round(trading_volume['AVG_STOCK_PRICE'], 2).alias('STOCK_PRICE')\n",
        "    )\n",
        "\n",
        "if not table_exists(session, database_name=database_name, schema_name=schema_name, table_name=table_name):\n",
        "    final_analysis.write.mode(\"overwrite\").save_as_table(table_name)\n",
        "    logger.info(f\"Successfully created {table_name}\")\n",
        "else:\n",
        "    cols_to_update = {c: final_analysis[c] for c in final_analysis.schema.names}\n",
        "    target = session.table(table_name)\n",
        "    target.merge(final_analysis, \n",
        "        (target['DATE'] == final_analysis['DATE']) & \n",
        "        (target['company_ticker'] == final_analysis['company_ticker']) &\n",
        "        (target['source'] == final_analysis['source']),\n",
        "        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n",
        "    logger.info(f\"Successfully updated {table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1a799d-a83f-4d53-905a-508cd8322522",
      "metadata": {
        "language": "python",
        "name": "py_process_csr",
        "title": "py_process_csr"
      },
      "outputs": [],
      "source": [
        "table_name = \"CLIENT_SECTOR_RISK\"\n",
        "\n",
        "transactions = session.table(\"TRANSACTIONS\")\n",
        "company = session.table(\"COMPANY_DATA\")\n",
        "\n",
        "client_positions = transactions.join(company, transactions['TICKER'] == company['company_ticker']) \\\n",
        "    .group_by(F.col('CLIENT'), F.col('sector')) \\\n",
        "    .agg(\n",
        "        F.sum(F.when(F.col('POSITION') == 'BUY', F.col('PRICE') * F.col('QUANTITY'))\n",
        "              .otherwise(F.col('PRICE') * F.col('QUANTITY') * -1)).alias('TOTAL_EXPOSURE'),\n",
        "        F.count_distinct('TICKER').alias('UNIQUE_TICKERS'),\n",
        "        F.sum('QUANTITY').alias('TOTAL_SHARES')\n",
        "    )\n",
        "\n",
        "client_totals = transactions.group_by(F.col('CLIENT')) \\\n",
        "    .agg(F.sum(F.when(F.col('POSITION') == 'BUY', F.col('PRICE') * F.col('QUANTITY'))\n",
        "               .otherwise(F.col('PRICE') * F.col('QUANTITY') * -1)).alias('TOTAL_PORTFOLIO_VALUE'))\n",
        "\n",
        "risk_analysis = client_positions.join(client_totals, 'CLIENT') \\\n",
        "    .select(\n",
        "        F.col('CLIENT'),\n",
        "        F.col('sector'),\n",
        "        F.round(F.col('TOTAL_EXPOSURE'), 2).alias('SECTOR_EXPOSURE'),\n",
        "        F.round((F.col('TOTAL_EXPOSURE') / F.col('TOTAL_PORTFOLIO_VALUE')) * 100, 2).alias('SECTOR_CONCENTRATION_PCT'),\n",
        "        F.col('UNIQUE_TICKERS'),\n",
        "        F.col('TOTAL_SHARES')\n",
        "    )\n",
        "\n",
        "if not table_exists(session, database_name=database_name, schema_name=schema_name, table_name=table_name):\n",
        "    risk_analysis.write.mode(\"overwrite\").save_as_table(table_name)\n",
        "    logger.info(f\"Successfully created {table_name}\")\n",
        "else:\n",
        "    cols_to_update = {c: risk_analysis[c] for c in risk_analysis.schema.names}\n",
        "    target = session.table(table_name)\n",
        "    target.merge(risk_analysis, (target['CLIENT'] == risk_analysis['CLIENT']) & (target['sector'] == risk_analysis['sector']),\n",
        "        [F.when_matched().update(cols_to_update), F.when_not_matched().insert(cols_to_update)])\n",
        "    logger.info(f\"Successfully updated {table_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b06e41-3330-43db-8026-02dfc8d8ecac",
      "metadata": {
        "codeCollapsed": true,
        "collapsed": false,
        "name": "md_debugging"
      },
      "source": [
        "## Debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8e0cb8-3c80-4bd7-87e2-88526e3377ff",
      "metadata": {
        "collapsed": false,
        "language": "sql",
        "name": "sql_debugging",
        "resultVariableName": "dataframe_2"
      },
      "outputs": [],
      "source": [
        "-- SELECT * FROM CLIENT_PORTFOLIO_METRICS LIMIT 10;\n",
        "-- SELECT * FROM PRESS_SENTIMENT_STOCK_IMPACT LIMIT 10;\n",
        "-- SELECT * FROM CLIENT_SECTOR_RISK LIMIT 10;"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    },
    "lastEditStatus": {
      "authorEmail": "jeremiah.hansen@snowflake.com",
      "authorId": "350843732387",
      "authorName": "JOHN",
      "lastEditTime": 1744841249680,
      "notebookId": "kss7vquztn5742emuo3m",
      "sessionId": "d322f813-fe43-46ce-a12e-8f6586bfa9bf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
